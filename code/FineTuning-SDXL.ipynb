{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09c673d",
   "metadata": {},
   "source": [
    "# Fine-Tuning StableDiffusion XL with DreamBooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aac13c",
   "metadata": {},
   "source": [
    "Over the past few years Generative AI models have popped up everywhere - from creating realistic responses to complex questions, to generating images and music to impress art critics around the globe. In this notebook we use the Hugging Face [Stable Diffusion XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model to create images from text prompts. You'll see how to import the SDXL model and use it to generate an image. \n",
    "\n",
    "From there, you'll see how you can fine-tune the model using [DreamBooth](https://huggingface.co/docs/diffusers/training/dreambooth), a method for easily fine-tuning a text-to-image model. We'll use a small number of photos of [Toy Jensen](https://blogs.nvidia.com/blog/2022/12/22/toy-jensen-jingle-bells/) in this notebook to fine-tune SDXL. This will allow us to generate new images that include Toy Jensen! \n",
    "\n",
    "After that, you'll have the chance to fine-tune the model on your own images. Perhaps you want to create an image of you at the bottom of the ocean, or in outer space? By the end of this notebook you will be able to! \n",
    "\n",
    "**IMPORTANT:** This project will utilize additional third-party open source software. Review the license terms of these open source projects before use. Third party components used as part of this project are subject to their separate legal notices or terms that accompany the components. You are responsible for confirming compliance with third-party component license terms and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee02dd",
   "metadata": {},
   "source": [
    "### Stable Diffusion XL Model\n",
    "\n",
    "First, we import the classes and libraries we need to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e603953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo \"BEFORE:: Virtual ENV - $VIRTUAL_ENV\"\n",
    "# !echo \"         PATH: $PATH\"\n",
    "# !python -m venv nvidia-usecase\n",
    "# !source nvidia-usecase/bin/activate\n",
    "# !echo \"AFTER:: Virtual ENV - $VIRTUAL_ENV\"\n",
    "# !echo \"        PATH: $PATH\"\n",
    "\n",
    "# !python -m venv nvidia-usecase\n",
    "# !source nvidia-usecase/bin/activate\n",
    "\n",
    "!pip uninstall -y -q optimum[\"onnxruntime\"] optimum[exporters] datasets evaluate\n",
    "!pip uninstall -y -q diffusers datasets evaluate huggingface-hub torch-model-archiver\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff19a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionXLPipeline, DiffusionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33488fe1",
   "metadata": {},
   "source": [
    "Next, from the Hugging Face `diffusers` library, we create a `StableDiffusionXLPipeline` object from the SDXL base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6042e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "\n",
    "!echo \"\"\n",
    "!echo \"Using [{model_id}] as the pre-trained model for this demo\"\n",
    "!echo \"\"\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True)\n",
    "pipe.to(\"cuda\")\n",
    "# pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e1a6e",
   "metadata": {},
   "source": [
    "Let's use the SDXL model to generate an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"toy jensen in space\"\n",
    "image = pipe(prompt=prompt).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0dad2",
   "metadata": {},
   "source": [
    "Hmmm, looks like the Hugging Face SDXL model doesn't know about Toy Jensen! Imagine that! \n",
    "\n",
    "âœ… Try using the SDXL model to generate some other images by editing the text in the first line of the cell above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5795a7",
   "metadata": {},
   "source": [
    "## Fine-Tuning the model with DreamBooth\n",
    "\n",
    "Fine-Tuning is used to train an existing Machine Learning Model, given new information. In our case, we want to teach the SDXL model about Toy Jensen. This will allow us to create the perfect image of Toy Jensen in Space!\n",
    "\n",
    "[DreamBooth](https://arxiv.org/abs/2208.12242) provides a way to fine-tune a text-to-image model using only a few images. Let's use this to tune our SDXL Model so that it knows about Toy Jensen!\n",
    "\n",
    "We have 8 photos of Toy Jensen in our dataset - let's take a look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "display(Image(filename='../data/toy-jensen/tj1.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets clone `diffusers` repo and use the correct versions of huggingface cli and torch\n",
    "!rm -rf repos/diffusers\n",
    "!mkdir -p repos\n",
    "\n",
    "!git clone https://github.com/huggingface/diffusers repos/diffusers\n",
    "!cd repos/diffusers && git checkout v0.21.4\n",
    "!pip install -q peft==0.9.0 huggingface_hub[cli,torch]==0.21.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10005ef",
   "metadata": {},
   "source": [
    "Now we can use Hugging Face and DreamBooth to fine-tune this model. To do this we create a config, then specify some flags like an instance prompt, a resolution and a number of training steps for the fine-tuning algorithm to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set PYTORCH_CUDA_ALLOC_CONF\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\n",
    "\n",
    "# Print total memory and other device properties\n",
    "print(torch.cuda.get_device_properties(0).total_memory)\n",
    "print(torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782e0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../models/tuned-toy-jensen\"\n",
    "output_lora_dir = output_dir + \"/lora\"   # pytorch_lora_weights.safetensors will be generated in this dir\n",
    "\n",
    "!echo \"\"\n",
    "!echo \"Using [{model_id}] as the pre-trained model for this demo\"\n",
    "!echo \"  output dir: [{output_dir}]\"\n",
    "!echo \"\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "!accelerate launch ./repos/diffusers/examples/dreambooth/train_dreambooth_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path={model_id}  \\\n",
    "  --instance_data_dir=../data/toy-jensen \\\n",
    "  --output_dir={output_lora_dir} \\\n",
    "  --mixed_precision=\"bf16\" \\\n",
    "  --instance_prompt=\"a photo of toy jensen\" \\\n",
    "  --resolution=768 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=100 \\\n",
    "  --seed=\"0\" \\\n",
    "  --resume_from_checkpoint=latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa7268",
   "metadata": {},
   "source": [
    "Now that the model is fine-tuned, let's tell our notebook where to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f899ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next 3 statements if re-running steps from this cell (in case of OOM and Kernel disconnect)\n",
    "model_id=\"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "output_dir = \"../models/tuned-toy-jensen\"\n",
    "output_lora_dir = output_dir + \"/lora\"   # pytorch_lora_weights.safetensors will be generated in this dir\n",
    "\n",
    "pipe = StableDiffusionXLPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(output_lora_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5ee85",
   "metadata": {},
   "source": [
    "Finally, we can use our fine-tuned model to create an image with Toy Jensen in it. Let's give it a go! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3ac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\"A picture of toy jensen in space\", num_inference_steps=75).images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb09409",
   "metadata": {},
   "source": [
    "Wow - look at him go! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = output_dir + \"/model\"\n",
    "print(f\"Saving fine-tuned model to: [{output_model_dir}]\")\n",
    "\n",
    "pipe.save_pretrained(output_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next 3 statements if re-running steps from this cell (in case of OOM and Kernel disconnect)\n",
    "# output_dir = \"../models/tuned-toy-jensen\"\n",
    "# output_model_dir = output_dir + \"/model\"\n",
    "# onnx_output_model_dir = output_dir + \"/sd_xl_onnx\"\n",
    "# mar_output_model_dir = output_dir + \"/mar/archive\"\n",
    "\n",
    "!echo\n",
    "!echo \"Data in model output dir [{output_model_dir}]\"\n",
    "!ls -lh {output_model_dir}\n",
    "\n",
    "!echo \"Fine-tuned model safetensor files in [{output_model_dir}]\"\n",
    "!find ../models/tuned-toy-jensen/model -name \"*safetensors\" -exec ls -lh {} \\;\n",
    "\n",
    "!echo \"Fine-tuned model onnx files in [{onnx_output_model_dir}]\"\n",
    "!find ../models/tuned-toy-jensen/sd_xl_onnx -name \"*onnx*\" -type f -exec ls -lh {} \\;\n",
    "\n",
    "# !echo \"Model ARchive file in [{mar_output_model_dir}]\"\n",
    "# !find ../models/tuned-toy-jensen/mar/archive -type f -exec ls -lh {} \\;\n",
    "\n",
    "# To see the whole model directory uncomment next statement\n",
    "# !find ../models/tuned-toy-jensen/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3252f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Import the transfer notebook to upload our pytorch model to minio\n",
    "# -------------------------------------------------\n",
    "%run ./Xfer-to-minio.ipynb\n",
    "\n",
    "# Uncomment next statement if running only this cell (in case of OOM and Kernel disconnect)\n",
    "output_model_dir = \"../models/tuned-toy-jensen/model\"\n",
    "prefix = \"torchserve-no_zip/fine-tuned\"\n",
    "s3_env: S3Env = init_env()\n",
    "minio_client: Minio = init_minio(s3_env)\n",
    "\n",
    "dir_model = MinioBucketMeta(model_data_dir=output_model_dir,\n",
    "                            bucket_name=s3_env.bucket_name,\n",
    "                            client=minio_client,\n",
    "                            prefix=prefix)\n",
    "upload_files(dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29721bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Generate the \"mar\" file for pytorch/torchserve\n",
    "# -------------------------------------------------\n",
    "\n",
    "# !./create-mar.sh \"/opt/app-root/src/demo/sgahlot-nvidia-usecase\"\n",
    "!./create-mar-without_zip.sh \"/opt/app-root/src/demo/sgahlot-nvidia-usecase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deadd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Import the transfer notebook to upload our pytorch model to minio\n",
    "# -------------------------------------------------\n",
    "%run ./Xfer-to-minio.ipynb\n",
    "\n",
    "# Uncomment next statement if running only this cell (in case of OOM and Kernel disconnect)\n",
    "output_dir = \"../models/tuned-toy-jensen\"\n",
    "# mar_output_model_dir = output_dir + \"/gen-mar/archive\"\n",
    "mar_output_model_dir = output_dir + \"/gen-mar/archive-without-zip\"\n",
    "prefix = \"torchserve-no_zip\"\n",
    "\n",
    "s3_env: S3Env = init_env()\n",
    "minio_client: Minio = init_minio(s3_env)\n",
    "\n",
    "dir_model = MinioBucketMeta(model_data_dir=mar_output_model_dir,\n",
    "                            bucket_name=s3_env.bucket_name,\n",
    "                            client=minio_client,\n",
    "                            prefix=prefix,\n",
    "                            exclude_dirs_set=['logs', 'workloads'])\n",
    "upload_files(dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Convert the model to a single ONNX file - NOT WORKING in OpenVino YET\n",
    "# -------------------------------------------------\n",
    "!pip install -q optimum[\"onnxruntime\"]\n",
    "!pip install -q optimum[exporters]\n",
    "!pip install --upgrade diffusers\n",
    "\n",
    "onnx_output_model_dir = output_dir + \"/sd_xl_onnx\"\n",
    "\n",
    "# If using a single safetensors file (created using the following command in output_dir)\n",
    "# python ../../code/repos/diffusers-main/scripts/convert_diffusers_to_original_sdxl.py \\\n",
    "#      --model_path ./model/ \\\n",
    "#      --checkpoint_path ./model-single/single.safetensors\n",
    "# output_model_dir = output_dir + \"/model-single\"\n",
    "\n",
    "!optimum-cli export onnx --model {output_model_dir} \\\n",
    "       --task text-to-image --device cuda --dtype fp16 \\\n",
    "       --framework pt --cache_dir ./sd_xl_onnx_cache \\\n",
    "       --monolith {onnx_output_model_dir}/\n",
    "       # Specify the following option if using a single onnx file\n",
    "       # --library diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Import the transfer notebook to upload onnx model to minio\n",
    "# -------------------------------------------------\n",
    "%run ./Xfer-to-minio.ipynb\n",
    "\n",
    "prefix = \"fine-tuned-onnx\"\n",
    "s3_env: S3Env = init_env()\n",
    "minio_client: Minio = init_minio(s3_env)\n",
    "\n",
    "dir_model = MinioBucketMeta(model_data_dir=onnx_output_model_dir,\n",
    "                            bucket_name=s3_env.bucket_name,\n",
    "                            client=minio_client,\n",
    "                            prefix=prefix)\n",
    "# print(f'\\n -> ONNX model data dir: {dir_model.model_data_dir}, bucket={dir_model.bucket_name}')\n",
    "upload_files(dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43775e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# To read the model from save model directory\n",
    "# -------------------------------------------------\n",
    "\n",
    "# # output_dir = \"../models/tuned-toy-jensen\"\n",
    "# # output_model_dir = output_dir + \"/model\"\n",
    "\n",
    "# pipe_1 = StableDiffusionXLPipeline.from_pretrained(output_model_dir, torch_dtype=torch.float16, use_safetensors=True, subfolder=\"scheduler\")\n",
    "# pipe_1 = pipe.to(\"cuda\")\n",
    "# pipe_1.load_lora_weights(output_lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# To read the model from a single safetensors file\n",
    "# -------------------------------------------------\n",
    "\n",
    "# !pip install omegaconf\n",
    "\n",
    "# output_dir = \"../models/tuned-toy-jensen\"\n",
    "\n",
    "# pipe = StableDiffusionXLPipeline.from_single_file(output_dir + \"/single.safetensors\", torch_dtype=torch.float16)\n",
    "# pipe = pipe.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
